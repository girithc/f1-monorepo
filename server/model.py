# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p5gr0RFWekH-Br-Af-XhtAtSAyv5VwHc
"""

# =====================================================
# Cell 1 â€” Robust paths for local project layout
# Works whether you run the notebook from repo root or /server
# =====================================================
import os, json
from pathlib import Path
import numpy as np
import pandas as pd

from sklearn.model_selection import GroupShuffleSplit
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error
from xgboost import XGBRegressor, DMatrix, train as xgb_train
import joblib

# Candidate roots to search (current dir, parent, grandparent)
_CWD = Path.cwd().resolve()
_candidates = [_CWD, _CWD.parent, _CWD.parent.parent]

def _find_project_root():
    for base in _candidates:
        if (base / "data" / "constructors.csv").exists():
            return base
    # last resort: look for a 'data' dir containing a few expected files
    for base in _candidates:
        d = base / "data"
        if d.exists() and (d / "results.csv").exists() and (d / "races.csv").exists():
            return base
    raise FileNotFoundError(
        "Could not locate project root with a ./data folder containing the F1 CSVs.\n"
        f"Checked: {[str(p) for p in _candidates]}"
    )

PROJECT_ROOT = _find_project_root()

# Define canonical paths relative to project root
DATA_DIR       = PROJECT_ROOT / "data"               # <-- your CSVs live here
ARTIFACTS_DIR  = PROJECT_ROOT / "artifacts"
HELPER_DIR     = PROJECT_ROOT / "server" / "helper"
MODEL_PATH     = ARTIFACTS_DIR / "finish_regressor_xgb.pkl"
SCHEMA_PATH    = ARTIFACTS_DIR / "schema_contract.json"

# Create output dirs if needed
ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)
HELPER_DIR.mkdir(parents=True, exist_ok=True)

print("ðŸ§­ Using directories:")
print("  PROJECT_ROOT :", PROJECT_ROOT)
print("  DATA_DIR     :", DATA_DIR)
print("  ARTIFACTS_DIR:", ARTIFACTS_DIR)
print("  HELPER_DIR   :", HELPER_DIR)

# Quick guard: ensure a few core CSVs exist
_required = ["constructors.csv", "drivers.csv", "races.csv", "circuits.csv", "results.csv", "pit_stops.csv"]
missing = [f for f in _required if not (DATA_DIR / f).exists()]
if missing:
    raise FileNotFoundError(f"Missing CSVs in {DATA_DIR}: {missing}\n"
                            "Make sure your Kaggle dataset is extracted into the project's ./data folder.")

# =====================================================
# 2. Load core CSVs from ./data
# =====================================================
constructors = pd.read_csv(DATA_DIR / "constructors.csv")
drivers      = pd.read_csv(DATA_DIR / "drivers.csv")
races        = pd.read_csv(DATA_DIR / "races.csv")
circuits     = pd.read_csv(DATA_DIR / "circuits.csv")
results      = pd.read_csv(DATA_DIR / "results.csv")
pit_stops    = pd.read_csv(DATA_DIR / "pit_stops.csv")

# --- circuit_laps.json (robust; derives laps from results.csv) ---
# For each race, compute the laps as the maximum laps completed by any classified finisher.
race_laps = (
    results[results['positionOrder'] > 0]           # only classified finishes
        .groupby('raceId', as_index=False)['laps']
        .max()
        .rename(columns={'laps': 'race_laps'})
)

# Join to races to get circuitId, then to circuits for name/country
circuit_meta = (
    races[['raceId', 'circuitId']]
        .merge(race_laps, on='raceId', how='left')
        .merge(circuits[['circuitId', 'name', 'country']], on='circuitId', how='left')
)

# Median race_laps per circuit â†’ avgLaps
circuit_meta = (
    circuit_meta
        .groupby(['circuitId', 'name', 'country'], as_index=False)['race_laps']
        .median()
        .rename(columns={'race_laps': 'avgLaps', 'name': 'name_circuit'})
)

# Fill any missing with overall median as a fallback
circuit_meta['avgLaps'] = circuit_meta['avgLaps'].fillna(circuit_meta['avgLaps'].median())

# Save for API
with open(HELPER_DIR / "circuit_laps.json", "w") as f:
    json.dump(circuit_meta.to_dict(orient='records'), f, indent=2)

# --- overtake_index.json ---
# use a dedicated variable name to avoid re-using 'res'
res_movement = results.merge(races[['raceId','circuitId','year']], on='raceId', how='left')
res_movement = res_movement[(res_movement['grid'] > 0) & (res_movement['positionOrder'] > 0)]
res_movement['pos_gain'] = res_movement['grid'] - res_movement['positionOrder']

race_movement = (res_movement.groupby(['raceId','circuitId'], as_index=False)['pos_gain']
                   .apply(lambda s: float(np.mean(np.abs(s)))))
race_movement.rename(columns={'pos_gain':'abs_movement'}, inplace=True)

circ_movement = race_movement.groupby('circuitId', as_index=False)['abs_movement'].mean()
vmin, vmax = circ_movement['abs_movement'].min(), circ_movement['abs_movement'].max()
circ_movement['overtakeIndex'] = (circ_movement['abs_movement'] - vmin) / (vmax - vmin + 1e-9)
overtake_index = circ_movement[['circuitId','overtakeIndex']]

# save helper file
with open(HELPER_DIR / "overtake_index.json", "w") as f:
    json.dump(overtake_index.to_dict(orient='records'), f, indent=2)

# =====================================================
# Car Performance Index â€” Qualifying *time*-based (constructor/year)
# =====================================================
qualifying = pd.read_csv(DATA_DIR / "qualifying.csv")

def _to_ms(x):
    if pd.isna(x):
        return np.nan
    s = str(x).strip()
    try:
        if ":" in s:
            m, rest = s.split(":")
            return (int(m) * 60.0 + float(rest)) * 1000.0
        return float(s) * 1000.0
    except:
        return np.nan

# Convert q1/q2/q3 to milliseconds and take the best per driver
for col in ["q1", "q2", "q3"]:
    qualifying[col + "_ms"] = qualifying[col].map(_to_ms)
qualifying["bestQ_ms"] = qualifying[["q1_ms", "q2_ms", "q3_ms"]].min(axis=1)

# Attach constructorId to each qualifying row (qualifying doesnâ€™t have it)
drv_cons = results[["raceId", "driverId", "constructorId"]].drop_duplicates()
q = (qualifying
     .merge(drv_cons, on=["raceId", "driverId"], how="left")
     .merge(races[["raceId", "year"]], on="raceId", how="left"))

# Ensure a single constructorId column after merges
if "constructorId" in q.columns:
    pass  # already good (comes from qualifying.csv)
else:
    # If duplicates exist, prefer qualifying's (usually _x), then fill from results' (_y)
    cand_x = "constructorId_x" if "constructorId_x" in q.columns else None
    cand_y = "constructorId_y" if "constructorId_y" in q.columns else None

    if cand_x or cand_y:
        q["constructorId"] = np.nan
        if cand_x:
            q["constructorId"] = q[cand_x]
        if cand_y:
            q["constructorId"] = q["constructorId"].fillna(q[cand_y])

        # clean up extras
        drop_cols = [c for c in [cand_x, cand_y] if c]
        q.drop(columns=drop_cols, inplace=True)
    else:
        raise RuntimeError("constructorId not found after merges; check input files/merges.")


# For each race & team, keep the team's best qualifying time (fastest driver of that team)
# IMPORTANT: include 'year' in the group keys so we don't lose it
team_best = (q.dropna(subset=["bestQ_ms"])
               .groupby(["raceId", "year", "constructorId"], as_index=False)["bestQ_ms"]
               .min())

# For each season, derive a constructor pace index from median of race-best times
cons_season = (team_best
               .groupby(["year", "constructorId"], as_index=False)["bestQ_ms"]
               .median()
               .rename(columns={"bestQ_ms": "med_bestQ_ms"}))

# Season-wise min-max to [0,1], where 1.0 = fastest in that season
season_minmax = cons_season.groupby("year")["med_bestQ_ms"].agg(["min", "max"]).reset_index()
cons_season = cons_season.merge(season_minmax, on="year", how="left")
rng = (cons_season["max"] - cons_season["min"]).replace(0, 1.0)
cons_season["carPerformanceIndex"] = 1.0 - ((cons_season["med_bestQ_ms"] - cons_season["min"]) / rng)

# Definitive CPI table for downstream merges
season_cons_pts = cons_season[["year", "constructorId", "carPerformanceIndex"]].copy()

# =====================================================
# 5. Pit features (count, durations, stints)
# =====================================================
ps = pit_stops.copy()
ps['milliseconds'] = ps['milliseconds'].fillna(0).astype(float)
agg = (ps.groupby(['raceId','driverId'], as_index=False)
         .agg(pit_count=('stop','count'),
              pit_total_duration=('milliseconds','sum'),
              pit_avg_duration=('milliseconds','mean'),
              first_pit_lap=('lap','min'),
              last_pit_lap=('lap','max')))

# --- Tire strategy aggressiveness proxy ---
agg['stints'] = (agg['pit_count'].fillna(0) + 1).clip(1, 5)
agg['tire_aggr_index'] = agg['stints'] / agg['pit_total_duration'].replace(0, np.nan)
agg['tire_aggr_index'] = agg['tire_aggr_index'].fillna(agg['tire_aggr_index'].median())

# =====================================================
# 6. Build training dataset
# =====================================================
Y = results.merge(races[['raceId','year','round','circuitId']], on='raceId', how='left')
Y = Y.merge(circuits[['circuitId','country']], on='circuitId', how='left')
Y = Y.merge(overtake_index, on='circuitId', how='left')
Y = Y.merge(agg, on=['raceId','driverId'], how='left')
Y = Y.merge(season_cons_pts, on=['year','constructorId'], how='left')

# Fill missing
for c in ['pit_count','pit_total_duration','pit_avg_duration','first_pit_lap','last_pit_lap']:
    Y[c] = Y[c].fillna(0)
    
Y['circuit_overtake_difficulty'] = Y['overtakeIndex'].fillna(Y['overtakeIndex'].median())
Y['carPerformanceIndex'] = Y['carPerformanceIndex'].fillna(Y['carPerformanceIndex'].median())

# --- first_stop_delta ---
Y = Y.merge(circuit_meta[['circuitId','avgLaps']], on='circuitId', how='left')
Y['first_stop_delta'] = np.where(
    (Y['avgLaps'].notna()) & (Y['first_pit_lap'] > 0),
    Y['first_pit_lap'] / Y['avgLaps'],
    0.0
)

rounds_per_year = Y.groupby('year', as_index=False)['round'].max().rename(columns={'round':'round_max'})
Y = Y.merge(rounds_per_year, on='year', how='left')
Y['season_progress'] = (Y['round'] - 1) / (Y['round_max'] - 1 + 1e-9)

TARGET = 'positionOrder'
# (Note: avgTireScore removed from FEATURES)
FEATURES = [
 'grid','pit_count','pit_total_duration','pit_avg_duration',
 'first_pit_lap','last_pit_lap','circuit_overtake_difficulty',
 'round','circuitId','country','carPerformanceIndex',
 'season_progress','first_stop_delta', 'tire_aggr_index'
]
dataset = Y[Y[TARGET] > 0][FEATURES + [TARGET]].copy()

# =====================================================
# 6b. Tire strategy features (per driver/race)
# =====================================================
pit_stops  = pd.read_csv(DATA_DIR / "pit_stops.csv")   # raceId, driverId, lap, duration, etc.
lap_times  = pd.read_csv(DATA_DIR / "lap_times.csv")   # raceId, driverId, lap, position, time

# Stint count = (#pit_stops + 1)
stints = (pit_stops.groupby(["raceId","driverId"], as_index=False)["stop"]
                   .count().rename(columns={"stop":"pitStops"}))
stints["tireStints"] = stints["pitStops"] + 1

# Average pit duration (ms)
pit_stops["duration_ms"] = pd.to_numeric(pit_stops["milliseconds"], errors="coerce")
pit_agg = (pit_stops.groupby(["raceId","driverId"], as_index=False)["duration_ms"]
                   .mean().rename(columns={"duration_ms":"avgPitMs"}))

# Tire features merge
tire_feats = stints.merge(pit_agg, on=["raceId","driverId"], how="outer")
tire_feats["avgPitMs"] = tire_feats["avgPitMs"].fillna(0)
tire_feats["tireStints"] = tire_feats["tireStints"].fillna(1)

# --- ðŸ‘‡ CRITICAL FIX: Merge tire_feats into Y so the columns actually exist! ---
# If we don't do this, 'tireStints' and 'avgPitMs' will be missing from training data
if "tireStints" not in Y.columns:
    Y = Y.merge(tire_feats, on=["raceId", "driverId"], how="left")
    Y["tireStints"] = Y["tireStints"].fillna(1)
    Y["avgPitMs"] = Y["avgPitMs"].fillna(0)
# --- ðŸ‘† END CRITICAL FIX ---

# =====================================================
# Target cleanup & outliers
# =====================================================
Y = Y.copy()

# Keep only classified finishers with real positions (1..20)
Y = Y[(Y["positionOrder"].notna()) & (Y["positionOrder"] > 0)]
Y["finish_pos"] = Y["positionOrder"].clip(1, 20)

# Remove grid==0 entries (no proper start position)
Y = Y[Y["grid"] > 0]

# Light winsorize on pit durations
if "avgPitMs" in Y.columns:
    Y["avgPitMs"] = Y["avgPitMs"].clip(
        lower=Y["avgPitMs"].quantile(0.01),
        upper=Y["avgPitMs"].quantile(0.99)
    )

# ðŸš¦ Filter chaotic races: require at least 16 classified finishers in the race
finishers = (results.assign(classified=(results['positionOrder'] > 0).astype(int))
                    .groupby('raceId', as_index=False)['classified'].sum()
                    .rename(columns={'classified': 'n_finishers'}))
Y = Y.merge(finishers, on='raceId', how='left')
Y = Y[Y['n_finishers'] >= 16]


# Restrict to modern era (Hybrid + DRS era)
Y = Y[Y['year'] >= 2014]

# =============================================================================
# [FIX] Sanitize Pit Data for "Pure Strategy" Training
# Goal: Train on what SHOULD happen (standard stops), not what DID happen (errors).
# =============================================================================

# 1. Calculate the Standard (Median) Pit Time for each Circuit
#    This tells us the mathematical cost of a stop at Monaco vs Silverstone.
circuit_pit_standards = (
    pit_stops
    .merge(races[['raceId', 'circuitId']], on='raceId')
    .groupby('circuitId')['milliseconds']
    .median()
    .rename('std_pit_loss_ms')
)

# 2. Merge this standard time into your main dataset Y
if 'std_pit_loss_ms' not in Y.columns:
    Y = Y.merge(circuit_pit_standards, on='circuitId', how='left')

# 3. Fill missing circuits with a global average (approx 22000ms)
Y['std_pit_loss_ms'] = Y['std_pit_loss_ms'].fillna(22000)

# 4. OVERWRITE the "Actual" columns with "Theoretical" columns
#    Now, the model sees the STRATEGIC cost (Count * Standard Time)
#    instead of the EXECUTION cost (Count * Random Mechanic Speed).

# Fix the total duration (used in features)
Y['pit_total_duration'] = Y['pit_count'] * Y['std_pit_loss_ms']

# Fix the average duration (used in features)
Y['pit_avg_duration']   = Y['std_pit_loss_ms']

# Fix the duplicate column from tire_feats if it exists
if 'avgPitMs' in Y.columns:
    Y['avgPitMs'] = Y['std_pit_loss_ms']

print("âœ… Data Sanitized: Pit durations standardized to remove execution noise.")

# =====================================================
# 9. Train/valid split (grouped by year)
# =====================================================
# (AvgTireScore removed from feature_cols)
feature_cols = [
    "grid","pit_count","pit_total_duration","pit_avg_duration",
    "first_pit_lap","last_pit_lap",
    "circuit_overtake_difficulty","round","circuitId","country",
    "carPerformanceIndex","tireStints","avgPitMs",
    "first_stop_delta", "tire_aggr_index"
]

# Keep only columns that actually exist
feature_cols = [c for c in feature_cols if c in Y.columns]
X = Y[feature_cols].copy()
y = Y["finish_pos"].astype(float)
groups = Y["raceId"]

gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
train_idx, valid_idx = next(gss.split(X, y, groups))

X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

# =====================================================
# 10. Model: ColumnTransformer + xgb.train (MAE + early stopping)
# =====================================================
numeric_features = [c for c in feature_cols if c not in ["country"]]
categorical_features = [c for c in feature_cols if c in ["country"]]

preprocess = ColumnTransformer(
    transformers=[
        ("num","passthrough", numeric_features),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
    ],
    remainder="drop"
)

# Fit preprocess, transform splits
prep_fitted = preprocess.fit(X_train, y_train)
X_train_t = prep_fitted.transform(X_train)
X_valid_t = prep_fitted.transform(X_valid)

dtrain = DMatrix(X_train_t, label=y_train.values)
dvalid = DMatrix(X_valid_t, label=y_valid.values)

# --- Define Monotonic Constraints ---
# 1  = Increasing (Value UP -> Pos UP/Worse)
# -1 = Decreasing (Value UP -> Pos DOWN/Better)
# 0  = No constraint
constraints = []
for col in numeric_features:
    if col == "grid":
        constraints.append(1)   # P20 start -> P20 finish
    elif col == "carPerformanceIndex":
        constraints.append(-1)  # High Perf -> Low Finish (P1)
    else:
        constraints.append(0)

monotone_str = "(" + ",".join(str(x) for x in constraints) + ")"

params = {
    "objective": "reg:absoluteerror",  # MAE
    "eta": 0.03,
    "max_depth": 6,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "min_child_weight": 4,
    "alpha": 1.0,
    "lambda": 3.0,
    "tree_method": "hist",
    "seed": 42,
    "monotone_constraints": monotone_str,
    "eval_metric": "mae",
}

watchlist = [(dtrain, "train"), (dvalid, "valid")]
booster = xgb_train(
    params,
    dtrain,
    num_boost_round=2000,
    evals=watchlist,
    early_stopping_rounds=100,
    verbose_eval=False
)

# A tiny wrapper so we can reuse in the rest of the notebook like a sklearn estimator
class BoosterWrapper:
    def __init__(self, booster, preprocessor):
        self.booster = booster
        self.preprocessor = preprocessor
    def predict(self, X):
        Xt = self.preprocessor.transform(X)
        return self.booster.predict(DMatrix(Xt))

# 'pipe' compatible object with .predict(X)
pipe = BoosterWrapper(booster, prep_fitted)

# RAW features only â€” the wrapper/pipeline will transform internally
pred_valid_raw = pipe.predict(X_valid)
pred_valid = np.clip(pred_valid_raw, 1, 20)

mae  = mean_absolute_error(y_valid, pred_valid)
rmse = np.sqrt(mean_squared_error(y_valid, pred_valid))
print(f"Validation MAE: {mae:.3f} | RMSE: {rmse:.3f}")

baseline_grid = np.clip(X_valid["grid"].values, 1, 20)
print("Baseline (finishâ‰ˆgrid) MAE:", mean_absolute_error(y_valid, baseline_grid))

# =====================================================
# Feature importances
# =====================================================
# 1) Identify model (Pipeline or raw Booster)
mdl = None
prep = None
try:
    # If you trained a Pipeline: pipe = Pipeline([("prep", preprocess), ("model", xgb)])
    mdl = pipe.named_steps["model"]
    prep = pipe.named_steps.get("prep", None)
except Exception:
    # If you stored the raw model in `pipe`
    mdl = pipe
    try:
        prep = preprocess  # your ColumnTransformer, if you kept it in a variable named 'preprocess'
    except NameError:
        prep = None

# 2) Get underlying Booster
try:
    booster = mdl.get_booster()     # XGBRegressor -> Booster
except Exception:
    booster = getattr(mdl, "booster", None) or mdl  # sometimes it's already a Booster

# 3) Importance dict
imp = booster.get_score(importance_type="gain")
if not imp:
    imp = booster.get_score(importance_type="weight")
if not imp:
    imp = booster.get_score(importance_type="cover")

# 4) Feature names
feat_names = None
if prep is not None:
    try:
        feat_names = list(prep.get_feature_names_out())
    except Exception:
        pass

if feat_names is None:
    # Infer count from keys (f0..fN-1)
    try:
        max_idx = max(int(k[1:]) for k in imp.keys() if str(k).startswith("f") and str(k[1:]).isdigit())
        n_feats = max_idx + 1
    except Exception:
        if "X_train" in globals():
            n_feats = X_train.shape[1]
        else:
            n_feats = len(imp)
    feat_names = [f"f{i}" for i in range(n_feats)]

# 5) Map importance to a dense array aligned to feature indices
scores = np.zeros(len(feat_names), dtype=float)
for k, v in imp.items():
    if k.startswith("f") and k[1:].isdigit():
        idx = int(k[1:])
        if 0 <= idx < len(scores):
            scores[idx] = float(v)
    else:
        # sometimes keys are real feature names
        try:
            idx = feat_names.index(k)
            scores[idx] = float(v)
        except ValueError:
            pass

# Normalize for readability
total = scores.sum()
if total > 0:
    scores = scores / total

pairs = list(zip(feat_names, scores))
pairs.sort(key=lambda x: x[1], reverse=True)
top = pairs[:20]

print("\nTop features:")
for n, w in top:
    print(f"{n:40s} {w:.4f}")

# Baseline 2: only car pace + overtake + circuit
pseudo = (
    21
    - 10 * X_valid['carPerformanceIndex'].fillna(0.5)
    - 3  * X_valid['circuit_overtake_difficulty'].fillna(0.5)
)
pseudo = np.clip(pseudo, 1, 20)
print("Pseudo baseline MAE:", mean_absolute_error(y_valid, pseudo))

# =====================================================
# 12. Save artifacts & helpers
# =====================================================
ARTIFACTS_DIR = Path("./artifacts")
ARTIFACTS_DIR.mkdir(exist_ok=True, parents=True)

# pack a tiny schema alongside the model (what server must provide)
serve_schema = {
    "numeric": numeric_features,       
    "categorical": categorical_features 
}

with open(ARTIFACTS_DIR / "serve_schema.json", "w") as f:
    json.dump(serve_schema, f, indent=2)

# save model (keep same filename if server expects it)
joblib.dump(pipe, ARTIFACTS_DIR / "finish_regressor_xgb_v2.pkl")
joblib.dump(pipe, ARTIFACTS_DIR / "finish_regressor_xgb.pkl")  # overwrite current for deployment
print("Saved model + serve_schema.json")